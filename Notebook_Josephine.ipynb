{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLÖpse\n",
    "The aim of the project is to simulate the realworld process of deploying machine learning models, using the concepts that we have discussed during the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://medium.com/analytics-vidhya/manage-your-machine-learning-lifecycle-with-mlflow-in-python-d678d5f3c682\n",
    "- https://towardsdatascience.com/experiment-tracking-with-mlflow-in-10-minutes-f7c2128b8f2c\n",
    "- https://towardsdatascience.com/tracking-ml-experiments-using-mlflow-7910197091bb\n",
    "- https://medium.com/hub-by-littlebigcode/mlops-how-mlflow-effortlessly-tracks-your-experiments-and-helps-you-compare-them-11da9be1fdb7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "\n",
    "UTC:\n",
    "Timestamp UTC seconds\n",
    "\n",
    "Temperature[C]:\n",
    "Air Temperature\n",
    "\n",
    "Humidity[%]:\n",
    "Air Humidity\n",
    "\n",
    "TVOC[ppb]:\n",
    "Total Volatile Organic Compounds; measured in parts per billion\n",
    "\n",
    "eCO2[ppm]:\n",
    "CO2 equivalent concentration; calculated from different values like TVOC\n",
    "\n",
    "Raw H2:\n",
    "Raw molecular hydrogen; not compensated (Bias, temperature, etc.)\n",
    "\n",
    "Raw Ethanol:\n",
    "Raw ethanol gas\n",
    "\n",
    "Pressure[hPa]:\n",
    "Air Pressure\n",
    "\n",
    "PM1.0:\n",
    "Particulate matter size < 1.0 µm (PM1.0). 1.0 µm < 2.5 µm (PM2.5)\n",
    "\n",
    "PM2.5:\n",
    "Particulate matter size < 1.0 µm (PM1.0). 1.0 µm < 2.5 µm (PM2.5)\n",
    "\n",
    "NC0.5:\n",
    "Number concentration of particulate matter. This differs from PM because NC gives the actual number of particles in the air. The raw NC is also classified by the particle size: < 0.5 µm (NC0.5); 0.5 µm < 1.0 µm (NC1.0); 1.0 µm < 2.5 µm (NC2.5);\n",
    "\n",
    "NC1.0:\n",
    "Number concentration of particulate matter. This differs from PM because NC gives the actual number of particles in the air. The raw NC is also classified by the particle size: < 0.5 µm (NC0.5); 0.5 µm < 1.0 µm (NC1.0); 1.0 µm < 2.5 µm (NC2.5);\n",
    "\n",
    "NC2.5:\n",
    "Number concentration of particulate matter. This differs from PM because NC gives the actual number of particles in the air. The raw NC is also classified by the particle size: < 0.5 µm (NC0.5); 0.5 µm < 1.0 µm (NC1.0); 1.0 µm < 2.5 µm (NC2.5);\n",
    "\n",
    "CNT:\n",
    "Sample counter\n",
    "\n",
    "Fire Alarm (Target):\n",
    "Ground truth is \"1\" if a fire is there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "# scikit-learn\n",
    "colors = sns.cubehelix_palette(n_colors=10, start=2.6, rot=0, dark=0.2, light=0.8)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# mlflow\n",
    "import mlflow\n",
    "# great expectations\n",
    "import great_expectations as gx\n",
    "# nannyml\n",
    "import nannyml as nml\n",
    "# termcolor\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow Experiment\n",
    "\n",
    "FIRST: \n",
    "1. open ***mlflow ui*** via terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "experiment_name = \"MLOps Project\"\n",
    "\n",
    "# create new experiment if not existing\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "# if experiment already exists, set experiment\n",
    "except:\n",
    "    mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get experiment id\n",
    "experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Data/smoke_detection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data\n",
    "\n",
    "To simulate multiple datasets we're going to order the dataset bei the *UTC* column and split it into 3 equal parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(dataframe: pd.DataFrame) -> tuple: #[pd.DataFrame, pd.DataFrame, pd.DataFrame]\n",
    "    # order dataframe by month\n",
    "    dataframe = dataframe.sort_values(by='UTC').reset_index(drop=True)\n",
    "    \n",
    "    # split dataframe into 3 equal sized datasets\n",
    "    dataframe_1 = dataframe.iloc[:int(len(dataframe)/3)]\n",
    "    dataframe_2 = dataframe.iloc[int(len(dataframe)/3):int(len(dataframe)/3)*2]\n",
    "    dataframe_3 = dataframe.iloc[int(len(dataframe)/3)*2:]\n",
    "    \n",
    "    return dataframe_1, dataframe_2, dataframe_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframes\n",
    "df1, df2, df3 = split_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [df1, df2, df3]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Rows: {df1.shape[0]}\\nColumns: {df1.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look at the data\n",
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics, data types and missing values\n",
    "def dataset_infos(dataframe: pd.DataFrame = dataframe) -> pd.DataFrame:\n",
    "    # descriptive statistics\n",
    "    descriptives = dataframe.describe(include='all').T\n",
    "\n",
    "    # data types column\n",
    "    descriptives.insert(loc=0, column='dtype', value=dataframe.dtypes)\n",
    "    \n",
    "    # missing values\n",
    "    missing = pd.concat([dataframe.isnull().sum(), dataframe.eq('').sum()], keys=['nulls','empty strings'], axis=1)\n",
    "    \n",
    "    return pd.merge(descriptives, missing, left_index=True, right_index=True)\n",
    "\n",
    "dataset_infos(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate rows\n",
    "print(f'Duplicated Rows: {df1.duplicated().sum()}')\n",
    "\n",
    "# # non-unique values\n",
    "# non_unique_count = (df1.shape[0] - df1.nunique()).sum()\n",
    "# print(f'Non-Unique Values: {non_unique_count}')\n",
    "\n",
    "# # unique values\n",
    "# print(f'Unique Values: {df1.nunique()}')\n",
    "\n",
    "# # missing values\n",
    "# print(f'Missing Values: {df1.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting a correlation matix to detect dependencies with the dependent variable\n",
    "\n",
    "# correlation_matrix = df1.corr()\n",
    "# mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# sns.heatmap(correlation_matrix, mask=mask, cmap=cmap, center=0, annot=True,\n",
    "#             square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "# plt.title(\"Correlation Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[df1['Fire Alarm']==1].head()\n",
    "# # properties, when fire alarm is executed\n",
    "# # 11956 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[df1['Fire Alarm']==0].head()\n",
    "# # properties, when fire alarm is executed-\n",
    "# # 8920 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fire_alarm_1 = df1[df1['Fire Alarm'] == 1]\n",
    "# df_fire_alarm_0 = df1[df1['Fire Alarm'] == 0]\n",
    "\n",
    "# sns.boxplot(x='Fire Alarm', y='Pressure[hPa]', data=df1, palette=colors).set(\n",
    "# title='Box Plot: Pressure[hPa]',\n",
    "# xlabel='Fire Alarm',\n",
    "# ylabel='Pressure[hPa]'\n",
    "# )\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df1['UTC'].unique()\n",
    "is_unique = len(unique_values) == len(df1['UTC'])\n",
    "print(is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df1['CNT'].unique()\n",
    "is_unique = len(unique_values) == len(df1['UTC'])\n",
    "print(is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target distribution (pie chart)\n",
    "def plot_target_distribution(dataframe: pd.DataFrame = dataframe, target='Fire Alarm'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.pie(dataframe[target].value_counts(), labels=dataframe[target].value_counts().index, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "    plt.title('Target Distribution')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_target_distribution(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_distribution(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_distribution(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df1.drop(columns=['Fire Alarm']), df1['Fire Alarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, stratify=y,random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x: pd.DataFrame, y: pd.Series = None, reset: bool = False) -> pd.DataFrame:\n",
    "  '''\n",
    "  Preprocessing function\n",
    "  Input: dataframe\n",
    "  Output: transformed data\n",
    "  '''\n",
    "\n",
    "  run_name = \"Preprocessing Pipeline\"\n",
    "\n",
    "  # preprocessing pipeline\n",
    "  pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('power_transformer', PowerTransformer())\n",
    "    ])\n",
    "    \n",
    "  # Search for preprocessing run\n",
    "  run = mlflow.search_runs(experiment_ids=experiment_id, filter_string=f\"tags.mlflow.runName = '{run_name}'\")\n",
    "  # get run id\n",
    "  run_id = run.iloc[0].run_id if not run.empty else None\n",
    "\n",
    "  # if preprocessing-run does not exist or retrain\n",
    "  if reset or run.empty:\n",
    "    # Delete the last run if it exists\n",
    "    if run_id is not None:\n",
    "        mlflow.delete_run(run_id)\n",
    "        print('Last run deleted')\n",
    "    # drop sample count (CNT) column\n",
    "    x = x.drop(columns=['CNT'])\n",
    "    # convert UTC column to datetime\n",
    "    x['Timestamp'] = pd.to_datetime(x['UTC'], unit='s')\n",
    "    # drop UTC column\n",
    "    x = x.drop(columns=['UTC'])\n",
    "    # create new columns\n",
    "    x['Hour'], x['WeekDay'] = x['Timestamp'].dt.hour, x['Timestamp'].dt.dayofweek\n",
    "    # drop timestamp column\n",
    "    x = x.drop(columns=['Timestamp'])\n",
    "    # fit & transform pipeline\n",
    "    data_preprocessed = pipeline.fit_transform(x)\n",
    "    # SMOTE\n",
    "    if y is not None:\n",
    "      # smote\n",
    "      smote = SMOTE(random_state=420)\n",
    "      # fit smote\n",
    "      x_smote, y_smote = smote.fit_resample(data_preprocessed, y)\n",
    "    # print fit and transform message\n",
    "    print('Preprocessing Pipeline: fit and transform')\n",
    "    # start run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "      # log pipeline\n",
    "      mlflow.sklearn.log_model(pipeline, 'PreprocessingPipeline')\n",
    "      # end run\n",
    "      mlflow.end_run()\n",
    "\n",
    "    return x_smote, y_smote\n",
    "    \n",
    "  # if preprocessing-run exists\n",
    "  else:\n",
    "    # load pipeline\n",
    "    pipeline = mlflow.sklearn.load_model(f'runs:/{run_id}/PreprocessingPipeline')\n",
    "    # drop sample count (CNT) column\n",
    "    x = x.drop(columns=['CNT'])\n",
    "    # convert UTC column to datetime\n",
    "    x['Timestamp'] = pd.to_datetime(x['UTC'], unit='s')\n",
    "    # drop UTC column\n",
    "    x = x.drop(columns=['UTC'])\n",
    "    # create new columns\n",
    "    x['Hour'], x['WeekDay'] = x['Timestamp'].dt.hour, x['Timestamp'].dt.dayofweek\n",
    "    # drop timestamp column\n",
    "    x = x.drop(columns=['Timestamp'])\n",
    "    # transform data\n",
    "    data_preprocessed = pipeline.transform(x)\n",
    "    # print transform message\n",
    "    print('Preprocessing Pipeline: transform')\n",
    "    # only return x (no smote on new data)\n",
    "    return data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "xtrain_prepro, ytrain_prepro = preprocessing(xtrain, ytrain, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test data\n",
    "xtest_prepro = preprocessing(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Multiple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit prediction model on train dataset\n",
    "def classifier_comparison(\n",
    "    xtrain: pd.DataFrame = xtrain_prepro,\n",
    "    ytrain: pd.DataFrame = ytrain_prepro,\n",
    "    xtest: pd.DataFrame = xtest_prepro,\n",
    "    ytest: pd.DataFrame = ytest,\n",
    "    clf: dict = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=420),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=420),\n",
    "        'Random Forest': RandomForestClassifier(random_state=420)\n",
    "        }\n",
    "    ):\n",
    "\n",
    "    run_name = \"Classifier Comparison\"\n",
    "\n",
    "    # start run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # build plot\n",
    "        fig, ax = plt.subplots(1, figsize=(10, 7))\n",
    "        # iterate over classifiers\n",
    "        for name, clf in clf.items():\n",
    "            # fit classifier\n",
    "            clf.fit(xtrain, ytrain)\n",
    "            # plot roc curve\n",
    "            RocCurveDisplay.from_estimator(clf, xtest, ytest, ax=ax, name=name)\n",
    "        ax.set_title('Receiver Operating Characteristic (ROC)')\n",
    "        ax.plot([0,1], [0,1], linestyle='--')\n",
    "        # save plot\n",
    "        plt.savefig('Plots/ROC Curves.png')\n",
    "        # log plot\n",
    "        mlflow.log_figure(fig, 'Plots/ROC Curves.png')\n",
    "        # end run\n",
    "        mlflow.end_run()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_comparison(xtrain=xtrain_prepro, ytrain=ytrain_prepro, xtest=xtest_prepro, ytest=ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(\n",
    "    x: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    xtest: pd.DataFrame = None,\n",
    "    ytest: pd.DataFrame = None,\n",
    "    clf = RandomForestClassifier(random_state=420),\n",
    "    param_grid: dict = {\n",
    "        'n_estimators': [100, 200, 300, 400, 500],\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        },\n",
    "    predict: bool = False\n",
    "    ):\n",
    "\n",
    "    # run name\n",
    "    clf_name = str(clf).split(\"(\")[0]\n",
    "    run_name = f\"Grid Search - {clf_name}\"\n",
    "\n",
    "    # Search for preprocessing run\n",
    "    run = mlflow.search_runs(experiment_ids=experiment_id, filter_string=f\"tags.mlflow.runName = '{run_name}'\")\n",
    "    # get run id\n",
    "    run_id = run.iloc[0].run_id if not run.empty else None\n",
    "\n",
    "    # if no predictions needed\n",
    "    # fit grid search to data and log model\n",
    "    if predict is False:\n",
    "        # delete the last run if it exists\n",
    "        if run_id is not None:\n",
    "            mlflow.delete_run(run_id)\n",
    "            \n",
    "        # start run\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            # initialize grid search\n",
    "            grid = GridSearchCV(clf, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "            # fit grid search\n",
    "            grid.fit(x, y)\n",
    "            # # log model\n",
    "            mlflow.sklearn.log_model(grid, f'GridSearch_{clf_name}')\n",
    "            # # log best parameters\n",
    "            mlflow.log_param('best_params', grid.best_params_)\n",
    "            # print message\n",
    "            print('GridSearchCV: fit')\n",
    "            # end run\n",
    "            mlflow.end_run()\n",
    "\n",
    "    elif predict:\n",
    "        # initialize grid search\n",
    "        grid = GridSearchCV(clf, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "        # fit grid search\n",
    "        grid.fit(x, y)\n",
    "        # predict\n",
    "        ypred = grid.predict(xtest)\n",
    "        # print classification report\n",
    "        print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression grid_params\n",
    "grid_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "\n",
    "grid_search(\n",
    "    x=xtrain_prepro,\n",
    "    y=ytrain_prepro,\n",
    "    xtest=xtest_prepro,\n",
    "    ytest=ytest,\n",
    "    clf=LogisticRegression(random_state=420),\n",
    "    param_grid=grid_params,\n",
    "    predict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree grid_params\n",
    "grid_params = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    }\n",
    "\n",
    "grid_search(\n",
    "    x=xtrain_prepro,\n",
    "    y=ytrain_prepro,\n",
    "    xtest=xtest_prepro,\n",
    "    ytest=ytest,\n",
    "    clf=DecisionTreeClassifier(random_state=420),\n",
    "    param_grid=grid_params,\n",
    "    predict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search(\n",
    "    x=xtrain_prepro,\n",
    "    y=ytrain_prepro,\n",
    "    xtest=xtest_prepro,\n",
    "    ytest=ytest,\n",
    "    clf=RandomForestClassifier(random_state=420),\n",
    "    predict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model with GridSearchCV-Parameters and all Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "        x: pd.DataFrame,\n",
    "        y: pd.DataFrame,\n",
    "        clf = RandomForestClassifier(random_state=420),\n",
    "        # reset: bool = False\n",
    "        ):\n",
    "    \n",
    "    # run name\n",
    "    clf_name = str(clf).split(\"(\")[0]\n",
    "    run_name = f\"Trained Model - {clf_name}\"\n",
    "\n",
    "    # check if grid search run exists\n",
    "    model_grid_search = f\"Grid Search - {clf_name}\"\n",
    "\n",
    "    # Search for grid search run\n",
    "    run = mlflow.search_runs(experiment_ids=experiment_id, filter_string=f\"tags.mlflow.runName = '{model_grid_search}'\")\n",
    "    # get run id\n",
    "    run_id = run.iloc[0].run_id if not run.empty else None\n",
    "    \n",
    "    # if grid search run does not exist\n",
    "    if run.empty:\n",
    "        print(f'No grid search run found for {clf_name}')\n",
    "        print('Model logged with default parameters')\n",
    "        # initialize default model\n",
    "        model = clf\n",
    "\n",
    "    # if grid search run exists\n",
    "    else:\n",
    "        print(f'Grid search run found for {clf_name}')\n",
    "        print('Model logged with optimal parameters')\n",
    "        # load grid search model\n",
    "        model = mlflow.sklearn.load_model(f'runs:/{run_id}/GridSearch_{clf_name}')\n",
    "\n",
    "    # start run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # fit model\n",
    "        model.fit(x, y)\n",
    "        # log model\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "        # end run\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test data\n",
    "x, y = preprocessing(x=pd.concat([xtrain, xtest], axis=0), y=pd.concat([ytrain, ytest], axis=0), reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression grid_params\n",
    "grid_params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "# grid search on full train dataset\n",
    "grid_search(x=x, y=y, clf=LogisticRegression(random_state=420), param_grid=grid_params)\n",
    "# train model on full train dataset\n",
    "training(x=x, y=y, clf=LogisticRegression(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree grid_params\n",
    "grid_params = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    }\n",
    "# grid search on full train dataset\n",
    "grid_search(x=x, y=y, clf=DecisionTreeClassifier(random_state=420), param_grid=grid_params)\n",
    "# train model on full train dataset\n",
    "training(x=x, y=y, clf=DecisionTreeClassifier(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest grid_params\n",
    "grid_params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    }\n",
    "# grid search on full train dataset\n",
    "grid_search(x=x, y=y, clf=RandomForestClassifier(random_state=420), param_grid=grid_params)\n",
    "# train model on full train dataset\n",
    "training(x=x, y=y, clf=RandomForestClassifier(random_state=420))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(x, y, clf = RandomForestClassifier(random_state=420)):\n",
    "\n",
    "    # get dataframe name\n",
    "    dataframe_name = [i for i in globals() if globals()[i] is x][0]\n",
    "\n",
    "    # run name\n",
    "    clf_name = str(clf).split(\"(\")[0]\n",
    "    run_name = f\"{clf_name} Prediction - {dataframe_name}\"\n",
    "\n",
    "    # check if classifier already got trained\n",
    "    trained_model_search = f\"Trained Model - {clf_name}\"\n",
    "\n",
    "    # search run\n",
    "    run = mlflow.search_runs(experiment_ids=experiment_id, filter_string=f\"tags.mlflow.runName = '{trained_model_search}'\")\n",
    "    \n",
    "    # if no trained model found\n",
    "    if run.empty:\n",
    "        print(f'No trained model found for {clf_name}')\n",
    "    # if trained model found\n",
    "    else:\n",
    "        print(f'Trained model found for {clf_name}\\n\\n')\n",
    "        # get run id\n",
    "        run_id = run.iloc[0].run_id if not run.empty else None\n",
    "        # load model (optimal)\n",
    "        model = mlflow.sklearn.load_model(f'runs:/{run_id}/model')\n",
    "\n",
    "    # start run\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # predict on new data\n",
    "        ypred = model.predict(x)\n",
    "        # log model accuracy\n",
    "        mlflow.log_metric(f'{dataframe_name}_accuracy', accuracy_score(y, ypred))\n",
    "        # log model precision\n",
    "        mlflow.log_metric(f'{dataframe_name}_precision', precision_score(y, ypred))\n",
    "        # log model recall\n",
    "        mlflow.log_metric(f'{dataframe_name}_recall', recall_score(y, ypred))\n",
    "        # log model f1-score\n",
    "        mlflow.log_metric(f'{dataframe_name}_f1_score', f1_score(y, ypred))\n",
    "        # log parameters\n",
    "        # mlflow.log_param(f'{dataframe_name}_params', model.get_params())\n",
    "        # log model\n",
    "        mlflow.sklearn.log_model(model, f'{dataframe_name}_model')\n",
    "        # print classification report\n",
    "        print(f'Classification Report:\\n\\n{classification_report(y, ypred)}\\n\\n')\n",
    "        # end run\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Installation\n",
    "\n",
    "- Open Anaconda Prompt Terminal\n",
    "- After satisfying !pip install great_expectations, run great_expectations init and confirm with 'Y'\n",
    "- Verify the installation by running the version !great_expectations --version\n",
    "- Our version used: version 0.16.13\n",
    "\n",
    "\n",
    "More information: https://docs.greatexpectations.io/docs/tutorials/quickstart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für meine Süßen zum Verstehen: https://www.youtube.com/watch?v=YLy4NsioUVI\n",
    "\n",
    "*Context*: A context in is the main object that manages the overall configuration and execution of the data expectations. It serves as a container for storing and organizing expectations, data sources, and validation results. The context allows to define, execute, and manage our data expectations.\n",
    "\n",
    "*Validator*: A validator is responsible for evaluating expectations on a given batch of data. Validators are used to validate data against a set of predefined expectations. They help to assess data quality, perform data validation, and monitor data pipelines.\n",
    "\n",
    "*Suite*: An Expectation Suite is a collection of expectations that define the desired properties and characteristics of our data. It serves as a set of rules against which your data can be validated. The suite contains a set of expectations that can be applied to one or more batches of data. \n",
    "\n",
    "*Batch*: A batch represents a subset of data that we want to evaluate against our expectations. It can be a collection of rows, a partitioned dataset, a file, a table, or any other logical grouping of data. Batches are used as inputs to validation processes and contain the data you want to validate.\n",
    "\n",
    "*Checkpoint*: A Checkpoint is a way to operationalize data validation using Expectation Suites. It allows you to define a pipeline-like flow for performing data validation on batches of data. It helps automate the validation process by defining the steps to be executed on data batches and tracking the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "import random\n",
    "# Select a random column index\n",
    "random_column = random.choice(df2.columns)\n",
    "\n",
    "# Drop the random column\n",
    "df2 = df2.drop(random_column, axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: Split data frame and save each one as a csv-file\n",
    "df1.to_csv('df_one.csv', index=False)\n",
    "df2.to_csv('df_two.csv', index=False)\n",
    "df3.to_csv('df_three.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "import great_expectations as gx\n",
    "from great_expectations.checkpoint import SimpleCheckpoint\n",
    "\n",
    "# Create data context\n",
    "context = gx.get_context()\n",
    "\n",
    "# Connect to data\n",
    "validator = context.sources.pandas_default.read_csv(\"Data/smoke_detection.csv\")\n",
    "\n",
    "# # get column mánmes from initial df, here df1\n",
    "# gx_columns = df1.get_table_columns()\n",
    "# # get dtypes from initial dataframe\n",
    "# gx_dtypes = df1.dtypes.apply(lambda x: str(x)).to_dict()\n",
    "\n",
    "column_names = [f\"{column_name}\" for column_name in validator.columns()]\n",
    "print(f\"Columns: {', '.join(column_names)}.\")\n",
    "print(validator.head(n_rows=5, fetch_all=False))\n",
    "\n",
    "# Create expectation suite\n",
    "suite = context.create_expectation_suite(\"smoke_detection_expectations\",overwrite_existing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use that data source for profiling, validation and documentation. More information regarding expectations, can be found here: https://legacy.docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Validator to create and run an Expectation\n",
    "validator.expect_table_columns_to_match_ordered_list(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_unique(\"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"Temperature[C]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"Humidity[%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"TVOC[ppb]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"eCO2[ppm]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"Raw H2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"Raw Ethanol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"PM1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"PM2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"NC0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"NC1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_to_exist(\"NC2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_unique(\"CNT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_in_set(\"Fire Alarm\",[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validator.validate()\n",
    "# print(results)\n",
    "\n",
    "# print(checkpoint_result)\n",
    "\n",
    "# print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and save our expectation suite\n",
    "print(validator.get_expectation_suite(discard_failed_expectations=False))\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "\n",
    "# Create a checkpoint\n",
    "checkpoint = SimpleCheckpoint(\n",
    "    'smoke_detection_checkpoint',\n",
    "    context,\n",
    "    validator=validator,\n",
    "    batches=[\n",
    "        {\n",
    "            \"batch_kwargs\": {\"path\": \"df_two.csv\"},\n",
    "            \"expectation_suite_names\": 'smoke_detection_expectations',\n",
    "        },\n",
    "        {\n",
    "            \"batch_kwargs\": {\"path\": \"df_three.csv\"},\n",
    "            \"expectation_suite_names\": 'smoke_detection_expectations',\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# # Create checkpoint\n",
    "# checkpoint = SimpleCheckpoint(\n",
    "#     \"smoke-detection\",\n",
    "#     context,\n",
    "#     validator=validator,\n",
    "# )\n",
    "\n",
    "# Run checkpoint to validate data \n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "# View results\n",
    "context.build_data_docs()\n",
    "\n",
    "validation_result_identifier = checkpoint_result.list_validation_result_identifiers()[0]\n",
    "context.open_data_docs(resource_identifier=validation_result_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test dataframes to Great Expectations' data objects\n",
    "test_data_gx = [\n",
    "    gx.from_pandas(df2),\n",
    "    gx.from_pandas(df3)\n",
    "]\n",
    "\n",
    "# Validate the test datasets against the expectation suite\n",
    "test_results = []\n",
    "for test_df in test_data_gx:\n",
    "    result = test_df.validate(expectation_suite=suite)\n",
    "    test_results.append(result)\n",
    "\n",
    "for i, result in enumerate(test_results):\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Test {i+1}: All Data Quality Tests are green!\")\n",
    "    else:\n",
    "        raise Exception(f\"Test {i+1}: Data Quality Tests failed.\")\n",
    "\n",
    "# Access the results\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"Test {i+1} Results:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://github.com/datarootsio/tutorial-great-expectations/blob/main/tutorial_great_expectations.ipynb\n",
    "- https://medium.com/@mostsignificant/python-data-validation-made-easy-with-the-great-expectations-package-8d1be266fd3f\n",
    "- https://www.linkedin.com/pulse/data-validation-great-expectations-ensure-your-meets-standards-kundi/?trk=pulse-article_more-articles_related-content-card\n",
    "- https://towardsdatascience.com/great-expectations-automated-testing-for-data-science-and-engineering-teams-1e7c78f1d2d5\n",
    "- https://towardsdatascience.com/a-great-python-library-great-expectations-6ac6d6fe822e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great expectation as data unit test tool\n",
    "# Lukas did: check column to exist/ datatypes, missing values, duplicates, distributions, outliers\n",
    "# For me to add: Column unique values of UTC and CNT, column value distribution of binary variable\n",
    "\n",
    "# I will not define the range, as we already look for outliers\n",
    "# For binary classification, a balanced dataset would ideally have a 50:50 distribution between the two classes\n",
    "# A threshold here could be between 30/70 and 20/80\n",
    "\n",
    "def great_expectations(check_dataframe, initial_dataframe=df1):\n",
    "\n",
    "    initial_df = gx.dataset.PandasDataset(initial_dataframe)\n",
    "    check_df = gx.dataset.PandasDataset(check_dataframe)\n",
    "    \n",
    "    # get column names from initial dataframe\n",
    "    gx_columns = initial_df.get_table_columns()\n",
    "    # get dtypes from initial dataframe\n",
    "    gx_dtypes = initial_df.dtypes.apply(lambda x: str(x)).to_dict()\n",
    "\n",
    "    # check if column are the same in all dataframes\n",
    "    column_check = check_df.expect_table_columns_to_match_ordered_list(column_list=gx_columns)\n",
    "    if column_check['success']:\n",
    "        print('Columns Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Columns Check: ', colored('Failed', 'red'))\n",
    "\n",
    "    # check if dtypes are the same in all dataframes\n",
    "    check_set = set()\n",
    "    for col, data_type in gx_dtypes.items():\n",
    "        datatype_check = check_df.expect_column_values_to_be_of_type(column=col, type_=data_type)\n",
    "        check_set.add(datatype_check['success'])\n",
    "    if all(check_set) == True:\n",
    "        print('Datatype Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Datatype Check: ', colored('Failed', 'red'))\n",
    "\n",
    "    # check if dataframe has missing values\n",
    "    check_set = set()\n",
    "    for col in gx_columns:\n",
    "        missing_values_check = check_df.expect_column_values_to_not_be_null(column=col)\n",
    "        check_set.add(missing_values_check['success'])\n",
    "    if all(check_set) == True:\n",
    "        print('Missing Values Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Missing Values Check: ', colored('Failed', 'red'))\n",
    "\n",
    "    # check if dataframe has duplicates\n",
    "    duplicates_check = check_df.expect_table_row_count_to_be_between(min_value=check_df.shape[0], max_value=check_df.shape[0])\n",
    "    if duplicates_check['success']:\n",
    "        print('Duplicates Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Duplicates Check: ', colored('Failed', 'red'))\n",
    "\n",
    "    # check if dataframe distribution is the same as in initial dataframe\n",
    "    check_set = set()\n",
    "    for col in gx_columns:\n",
    "        distribution_check = check_df.expect_column_kl_divergence_to_be_less_than(column=col, partition_object={'values': initial_df[col].value_counts().index.tolist(), 'weights': initial_df[col].value_counts(normalize=True).tolist()}, threshold=0.6)\n",
    "        check_set.add(distribution_check['success'])\n",
    "    if all(check_set) == True:\n",
    "        print('Distribution Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Distribution Check: ', colored('Failed', 'red'))\n",
    "\n",
    "    # check if dataframe has outliers\n",
    "    check_set = set()\n",
    "    for col in gx_columns:\n",
    "        outliers_check = check_df.expect_column_values_to_be_between(column=col, min_value=initial_df[col].min(), max_value=initial_df[col].max())\n",
    "        check_set.add(outliers_check['success'])\n",
    "    if all(check_set) == True:\n",
    "        print('Outlier Check: ', colored('OK', 'green'))\n",
    "    else:\n",
    "        print('Outlier Check: ', colored('Failed', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_expectations(check_dataframe=df2, initial_dataframe=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_prepro, df2_y = preprocessing(df2.drop(columns=['Fire Alarm'])), df2['Fire Alarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df2_prepro, df2_y, clf=LogisticRegression(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df2_prepro, df2_y, clf=DecisionTreeClassifier(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df2_prepro, df2_y, clf=RandomForestClassifier(random_state=420))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_expectations(check_dataframe=df3, initial_dataframe=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_prepro, df3_y = preprocessing(df3.drop(columns=['Fire Alarm'])), df3['Fire Alarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df3_prepro, df3_y, clf=LogisticRegression(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df3_prepro, df3_y, clf=DecisionTreeClassifier(random_state=420))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_new_data(df3_prepro, df3_y, clf=RandomForestClassifier(random_state=420))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
